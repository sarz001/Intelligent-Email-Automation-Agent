{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3qCe3is_lh6"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "\n",
    "%pip install -U transformers datasets accelerate peft trl bitsandbytes\n",
    "%pip install google-api-python-client==1.7.2 google-auth==2.14.1 google-auth-httplib2==0.0.3 google-auth-oauthlib==0.4.1\n",
    "%pip install langgraph==0.0.37\n",
    "%pip install typing_extensions\n",
    "\n",
    "!pip install sentencepiece==0.1.99\n",
    "\n",
    "!pip install html2text\n",
    "%pip install \"huggingface_hub>=0.34.0,<1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQdpn5Tdk7Xg"
   },
   "source": [
    "RESTART SESSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIAWBv75qinH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5F78ZjGZb-U"
   },
   "outputs": [],
   "source": [
    "from email.mime.text import MIMEText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orMzM7DtK1zY"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0yZkdbSRohs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import re\n",
    "import html2text\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Google API & Auth\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# LangGraph for the agent structure\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Set, Dict\n",
    "\n",
    "# Transformers, TRL & PEFT for the AI model\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset, load_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikhKkKgZ32el"
   },
   "source": [
    "\n",
    "**The Agent State**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-PHhfiI3z-B"
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict, total=False):\n",
    "    \"\"\"Defines the memory or state of our agent's workflow.\"\"\"\n",
    "    unread_emails: List[Dict]\n",
    "    current_email_id: str\n",
    "    current_email_content: str\n",
    "    current_email_sender: str\n",
    "    current_email_subject: str\n",
    "    current_email_message_id: str\n",
    "    classification: str\n",
    "    total_emails: int\n",
    "    email_index: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUOat9QY4Ko2"
   },
   "source": [
    "**FINETUNING THE BASE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuseh7y0RrrR"
   },
   "outputs": [],
   "source": [
    "EXTRACTION_SYSTEM_PROMPT = \"You are an expert at extracting tasks and deadlines. Respond with a JSON object containing 'task' and 'deadline' keys, or 'null'.\"\n",
    "\n",
    "CLASSIFICATION_SYSTEM_PROMPT = \"You are an email classification expert. Your job is to classify the email's intent. Choose ONLY one of the following categories: deadline, question, other.\"\n",
    "\n",
    "def format_prompt_for_training(sample: dict) -> str:\n",
    "    \"\"\"\n",
    "    A smart formatting function that uses the correct system prompt\n",
    "    based on the type of response in the dataset.\n",
    "    \"\"\"\n",
    "    response = sample['response']\n",
    "    system_prompt = EXTRACTION_SYSTEM_PROMPT if response.startswith('{') else CLASSIFICATION_SYSTEM_PROMPT\n",
    "\n",
    "    return f\"\"\"<|system|>\n",
    "{system_prompt}<|end|>\n",
    "<|user|>\n",
    "{sample['text']}<|end|>\n",
    "<|assistant|>\n",
    "{response}<|end|>\"\"\"\n",
    "\n",
    "def create_classification_prompt(email_content: str) -> str:\n",
    "    \"\"\"Creates the prompt for the classification tool, now using a consistent system message.\"\"\"\n",
    "    return f\"\"\"<|system|>\n",
    "{CLASSIFICATION_SYSTEM_PROMPT}<|end|>\n",
    "<|user|>\n",
    "{email_content}<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "def create_extraction_prompt(email_content: str) -> str:\n",
    "    \"\"\"Creates the prompt for the extraction tool, now using a consistent system message.\"\"\"\n",
    "    return f\"\"\"<|system|>\n",
    "{EXTRACTION_SYSTEM_PROMPT}<|end|>\n",
    "<|user|>\n",
    "{email_content}<|end|>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 713,
     "referenced_widgets": [
      "74dc81b780d14bc086a31ab21e3c9686",
      "06b4b497d0114b3da460550a85a049c8",
      "3cf7984dc0f14b6ab199468ec824684b",
      "e3f8a1266d6940ac9c0020272d1fb78a",
      "78e597b77bfa469d9f739576bc84c543",
      "47d03fa01d35412298db6b5fb2bf54c6",
      "35a332c2bfac48a29418f7c6456cb4ea",
      "cb1fa4c579b4488cb571077135ae3220",
      "0bee612eda684296b545ceac685b290f",
      "d000b00d283744a4b4ea8744780ad24a",
      "920145e3bb0740778de79ac7067a48e8",
      "7d1a822fc823456caf5ce27827df1bb9",
      "fb8ffc59821c43a9882a1550b8c003f9",
      "d43c6fd39b5044a4ad041ad0e83fafc2",
      "27a054f8dfe642aa9cc0bec716509725",
      "34db43cefdbd4c8e9d6c2dddf8d2de62",
      "d409c940cdaf42b19193aac098fa9ece",
      "b25e11f0a20b463cb14e9eff5c0cc92f",
      "085294bf03d940c9be48422c67c9ddba",
      "c8e11d7ee34a4782b93aa17090f50108",
      "ee1d49976fa4443d8760c03db3b07adf",
      "168fbf76ad0545cc98b8010f8e9db56c",
      "78056cc8d9334f9abbc75c3d5014b794",
      "f3d8119fcba7453cb516bd247a8f7c7a",
      "18eeee1707b040c79cfc83a3f8d1e106",
      "bd971483b3c2448a81a1c9754f823952",
      "f3612c6866ed4e01a0445371cc1dbd78",
      "476e3d89d78a452bb2036b92590391d3",
      "b1bd9a667ee2428a9384c9073a471076",
      "d158719f488e4b50b5b0eded49665232",
      "2d582357e4074ffdac99736dc00ab262",
      "7f5d80687dec41abbbb19f6a2d48fbdc",
      "b341ca0006a3444bb25d0a4a7df6f014",
      "8d6fefa72bca4e18837a965710dcf9d6",
      "9154617111fc484da6e47e26344fbe1b",
      "cf89d81ed43146a5880d1c19c8691078",
      "68421c6708ed4a35bdc08074dc053123",
      "66c5329e35a546b0afc39b1e49ef3ded",
      "97199c171d3045e7892ee034c764be09",
      "0ed4c50087814ed78911f29f37d7b1b1",
      "f3d5e18b6fd0426ab29598b2f66bcd3a",
      "2c00063e74744df684786f2bbfaf483d",
      "d86efecac8024917b651a7b929e6a793",
      "6b0075ea11f64bcf88a7660d3e426549",
      "f4140b86ab2348b6afafe6829932f96f",
      "72e79cdb65774655ac1b3c39b0d6e558",
      "0906ecf500134633941f07a1f0fa355d",
      "fe4c1166fd7b4bf297abe95bd0290ef8",
      "61f08558513c4a6c9cd8d67d211a1979",
      "eeb3c1b464344cb9a2a5abb8a56327b9",
      "90c0212e5d4042a398f76ee5f5c49040",
      "ec61458c57424d87849e21f5415f416e",
      "2cf5ead4569345d4bc69b217425ce2e8",
      "6ce62f5661d840a99306a40cf2ad1b5f",
      "5f56316907ee48a6b1b46708aa0e53b9",
      "045e2e738fb74b3aa4472e30adb9e843",
      "fe334724cd304f169d779c00bc35605f",
      "2cd5a25cecec4a7196e14cec710bd412",
      "9e01bbf7365943b8be5283e7ff781fc8",
      "ed7d8d8a672241bbbba5ed7efe9612b5",
      "a90bb83c782749618e2816a6e7b1663a",
      "5247df0380134bc4a1710dc20d92f3a3",
      "13df8d83870e451981083f164b0cf8d7",
      "f8ce2109ac6b4ecaa91428e26aab3e74",
      "4ce8fe1d9d9c45d2b4093f2ac3b1fb2d",
      "27e7f02a27be44719a7b321a76b25ce3"
     ]
    },
    "id": "Z8qsyZcjR0U3",
    "outputId": "b7f9800f-7430-4f96-efc4-da9a4f7dfad5"
   },
   "outputs": [],
   "source": [
    "def run_fine_tuning():\n",
    "    \"\"\"This function fine-tunes the model on the combined dataset.\"\"\"\n",
    "    print(\"ðŸš€ Starting multi-task fine-tuning process...\")\n",
    "    MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "    DATASET_PATH = \"dataset.jsonl\"\n",
    "    ADAPTERS_PATH = \"./phi3-mini-deadline-extractor-adapters\"\n",
    "\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        raise FileNotFoundError(f\"{DATASET_PATH} not found. Please upload your training data.\")\n",
    "    dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=quant_config, device_map=\"auto\", trust_remote_code=True)\n",
    "    base_model.config.use_cache = False\n",
    "\n",
    "    peft_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=\"all-linear\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=6,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        logging_steps=5,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        max_grad_norm=0.3,\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=base_model,\n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_config,\n",
    "        formatting_func=format_prompt_for_training,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    print(\"â³ Training model on both tasks...\")\n",
    "    trainer.train()\n",
    "    print(\"âœ… Fine-tuning complete.\")\n",
    "    trainer.model.save_pretrained(ADAPTERS_PATH)\n",
    "    print(f\"âœ… Multi-task adapters saved to {ADAPTERS_PATH}\")\n",
    "\n",
    "\n",
    "run_fine_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWRfu6Dz4TN4"
   },
   "source": [
    "**AUTHENTICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "7c2d3891ed6d4867af524cb23929e3dc",
      "51b2c679351541ea9cd0b8aeb61449d1",
      "85deb99956c74be0b009ad2d988c6b0c",
      "188eb8fdf0964ded81847dea306beace",
      "69337c87bdda410a8002c04667735afc",
      "ec80e8a8d0bc46f7950be9d684385440",
      "01ae4a918fee4af4854df5e48bad0d0f",
      "740066d9c8cb47629c734fd44f8f54ce",
      "38f2ad18331f4968baf02143dd1945a9",
      "46e6bf48898343e8ad204f5cff4781d0",
      "36947bff8caf461e937376373c953464"
     ]
    },
    "id": "dxF8cnoXR22T",
    "outputId": "4cb4157d-46b5-48eb-a7ea-1ecfdd17e0ff"
   },
   "outputs": [],
   "source": [
    "# --- GLOBAL VARIABLES ---\n",
    "gmail_service = None\n",
    "calendar_service = None\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "# Import Request for Google Auth\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "def initialize_globals():\n",
    "    \"\"\"Authenticates and loads all necessary services and the model into global variables.\"\"\"\n",
    "    global gmail_service, calendar_service, model, tokenizer\n",
    "\n",
    "    # --- Authenticate with Google ---\n",
    "    print(\"ðŸ”‘ Authenticating Google Services...\")\n",
    "    SCOPES = ['https://www.googleapis.com/auth/gmail.modify', 'https://www.googleapis.com/auth/calendar.events']\n",
    "    TOKEN_FILE = 'token.json'\n",
    "    CREDS_FILE = 'credentials.json'\n",
    "    creds = None\n",
    "    if os.path.exists(TOKEN_FILE):\n",
    "        creds = Credentials.from_authorized_user_file(TOKEN_FILE, SCOPES)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(CREDS_FILE, SCOPES)\n",
    "            creds = flow.run_console()\n",
    "        with open(TOKEN_FILE, 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "    gmail_service = build(\"gmail\", \"v1\", credentials=creds)\n",
    "    calendar_service = build(\"calendar\", \"v3\", credentials=creds)\n",
    "    print(\"âœ… Google Services are ready.\")\n",
    "\n",
    "    # --- Load Fine-Tuned Model ---\n",
    "    print(\"ðŸ§  Loading fine-tuned model...\")\n",
    "    MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "    ADAPTERS_PATH = \"./phi3-mini-deadline-extractor-adapters\"\n",
    "    if not os.path.exists(ADAPTERS_PATH):\n",
    "        raise FileNotFoundError(f\"Adapter path {ADAPTERS_PATH} not found. Run fine-tuning first.\")\n",
    "\n",
    "    quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=quant_config, device_map=\"auto\", trust_remote_code=True)\n",
    "    model = PeftModel.from_pretrained(base_model, ADAPTERS_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"âœ… Model is loaded and ready.\")\n",
    "\n",
    "# --- Run the initialization once at the start ---\n",
    "initialize_globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_17vcESagFk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FOjVCCv4ZWa"
   },
   "source": [
    "***FUNCTIONS TO READ, EDIT , SEND MAILS and CREATE EVENTS IN GOOGLE CALENDAR***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlDAJDHYYXmM"
   },
   "outputs": [],
   "source": [
    "\n",
    "def _create_and_send_reply(to: str, subject: str, in_reply_to: str, reply_body: str):\n",
    "    \"\"\"Creates an email message and sends it via the Gmail API.\"\"\"\n",
    "    print(\"   - Composing reply...\")\n",
    "    message = MIMEText(reply_body)\n",
    "    message['to'] = to\n",
    "    message['from'] = 'me'\n",
    "    message['subject'] = f\"Re: {subject}\"\n",
    "    message['In-Reply-To'] = in_reply_to\n",
    "    message['References'] = in_reply_to\n",
    "    encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n",
    "    body = {'raw': encoded_message}\n",
    "    try:\n",
    "        sent_message = gmail_service.users().messages().send(userId='me', body=body).execute()\n",
    "        print(f\"   - âœ… Success! Reply sent with ID: {sent_message['id']}\")\n",
    "    except HttpError as error:\n",
    "        print(f\"   - âŒ An API error occurred while sending the reply: {error}\")\n",
    "\n",
    "def draft_and_send_reply(state: AgentState) -> AgentState:\n",
    "    \"\"\"Tool 2: Drafts AND SENDS a reply for emails classified as questions.\"\"\"\n",
    "    print(\"ðŸ› ï¸ Using Tool: Draft and Send Reply\")\n",
    "\n",
    "    # Define the reply body\n",
    "    reply_body = \"Thank you for your question. I am an AI assistant and have forwarded your request to the appropriate team member. They will get back to you shortly.\"\n",
    "    print(f\"   - Sending Reply: '{reply_body}'\")\n",
    "\n",
    "    # ** FIX: Call the new helper to actually send the email **\n",
    "    _create_and_send_reply(\n",
    "        to=state[\"current_email_sender\"],\n",
    "        subject=state[\"current_email_subject\"],\n",
    "        in_reply_to=state[\"current_email_message_id\"],\n",
    "        reply_body=reply_body\n",
    "    )\n",
    "\n",
    "    _mark_email_as_read(state[\"current_email_id\"])\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-3sKQ7EiEyW"
   },
   "outputs": [],
   "source": [
    "def _mark_as_processed_and_read(msg_id: str):\n",
    "    \"\"\"Applies the 'Processed' label and marks the email as read.\"\"\"\n",
    "    try:\n",
    "        label_name = \"Agent-Processed\"\n",
    "        # Ensure the label exists, create if not (optional, good for robustness)\n",
    "        labels = gmail_service.users().labels().list(userId='me').execute()\n",
    "        label_ids = [l['id'] for l in labels['labels'] if l['name'] == label_name]\n",
    "        if not label_ids:\n",
    "            label = gmail_service.users().labels().create(userId='me', body={'name': label_name}).execute()\n",
    "            label_id = label['id']\n",
    "        else:\n",
    "            label_id = label_ids[0]\n",
    "\n",
    "        gmail_service.users().messages().modify(\n",
    "            userId='me',\n",
    "            id=msg_id,\n",
    "            body={'addLabelIds': [label_id], 'removeLabelIds': ['UNREAD']}\n",
    "        ).execute()\n",
    "        print(f\"   - Marked email {msg_id} as processed and read.\")\n",
    "    except HttpError as e:\n",
    "        print(f\"   - âŒ ERROR: Failed to modify email {msg_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysKxNgTqiBnf"
   },
   "outputs": [],
   "source": [
    "def _label_as_processed(msg_id: str):\n",
    "    \"\"\"Applies the 'Processed' label but leaves the email unread.\"\"\"\n",
    "    try:\n",
    "        label_name = \"Agent-Processed\"\n",
    "        labels = gmail_service.users().labels().list(userId='me').execute()\n",
    "        label_ids = [l['id'] for l in labels['labels'] if l['name'] == label_name]\n",
    "        if not label_ids:\n",
    "            label = gmail_service.users().labels().create(userId='me', body={'name': label_name}).execute()\n",
    "            label_id = label['id']\n",
    "        else:\n",
    "            label_id = label_ids[0]\n",
    "\n",
    "        gmail_service.users().messages().modify(\n",
    "            userId='me',\n",
    "            id=msg_id,\n",
    "            body={'addLabelIds': [label_id]}\n",
    "        ).execute()\n",
    "        print(f\"   - Marked email {msg_id} as processed (remains unread).\")\n",
    "    except HttpError as e:\n",
    "        print(f\"   - âŒ ERROR: Failed to apply label to {msg_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NIZOmGJanSO"
   },
   "outputs": [],
   "source": [
    "def ignore_email(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    NEW TOOL: Ignores the email by applying the processed label but leaving it unread.\n",
    "    \"\"\"\n",
    "    print(\"   - Action: Ignoring Email\")\n",
    "    print(f\"   - Email {state['current_email_id']} is not a deadline or question. Leaving unread.\")\n",
    "    _label_as_processed(state[\"current_email_id\"]) # Apply label to prevent re-processing\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YNdfaUFStaZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_email_details(msg_id: str) -> dict:\n",
    "    \"\"\"Fetches the content and key headers of a single email, returning them in a dictionary.\"\"\"\n",
    "    try:\n",
    "        msg = gmail_service.users().messages().get(userId=\"me\", id=msg_id, format='full').execute()\n",
    "        headers = msg['payload'].get(\"headers\", [])\n",
    "\n",
    "        subject = next((h['value'] for h in headers if h['name'].lower() == 'subject'), \"No Subject\")\n",
    "        sender_header = next((h['value'] for h in headers if h['name'].lower() == 'from'), \"\")\n",
    "        message_id = next((h['value'] for h in headers if h['name'].lower() == 'message-id'), \"\")\n",
    "\n",
    "        sender_email_match = re.search(r'<(.+?)>', sender_header)\n",
    "        if sender_email_match:\n",
    "            sender = sender_email_match.group(1)\n",
    "        else:\n",
    "            sender = sender_header # Fallback for simple email addresses\n",
    "\n",
    "        body = \"\"\n",
    "        if \"parts\" in msg['payload']:\n",
    "            for part in msg['payload']['parts']:\n",
    "                if part['mimeType'] == 'text/plain':\n",
    "                    body_data = part['body'].get('data', '')\n",
    "                    body = base64.urlsafe_b64decode(body_data.encode('ASCII')).decode('utf-8')\n",
    "                    break\n",
    "            if not body:\n",
    "                 for part in msg['payload']['parts']:\n",
    "                    if part['mimeType'] == 'text/html':\n",
    "                        html_data = part['body'].get('data', '')\n",
    "                        html_body = base64.urlsafe_b64decode(html_data.encode('ASCII')).decode('utf-8')\n",
    "                        body = html2text.html2text(html_body)\n",
    "                        break\n",
    "        elif msg['payload'].get('body', {}).get('data'):\n",
    "            body_data = msg['payload']['body']['data']\n",
    "            body = base64.urlsafe_b64decode(body_data.encode('ASCII')).decode('utf-8')\n",
    "\n",
    "        return {\n",
    "            \"body\": f\"Subject: {subject}\\n\\n{body.strip()}\",\n",
    "            \"sender\": sender,\n",
    "            \"subject\": subject,\n",
    "            \"message_id\": message_id\n",
    "        }\n",
    "    except HttpError as e:\n",
    "        print(f\"ERROR fetching email details: {e}\")\n",
    "        return {}\n",
    "\n",
    "def _create_calendar_event(task: str, deadline_str: str) -> bool:\n",
    "    \"\"\"Creates a calendar event and returns True on success, False on failure.\"\"\"\n",
    "    try:\n",
    "        deadline_dt = datetime.fromisoformat(deadline_str)\n",
    "\n",
    "        event = {\n",
    "            \"summary\": task,\n",
    "            \"start\": {\"dateTime\": deadline_dt.isoformat(), \"timeZone\": \"Asia/Kolkata\"},\n",
    "            \"end\": {\"dateTime\": (deadline_dt + timedelta(hours=1)).isoformat(), \"timeZone\": \"Asia/Kolkata\"},\n",
    "        }\n",
    "        created_event = calendar_service.events().insert(calendarId=\"primary\", body=event).execute()\n",
    "\n",
    "        if created_event and 'htmlLink' in created_event:\n",
    "            print(f\"   - âœ… Success! Calendar event created: {created_event.get('htmlLink')}\")\n",
    "            return True # Signal success\n",
    "        else:\n",
    "            print(\"   - âš ï¸ API call succeeded but no event was created.\")\n",
    "            return False # Signal failure\n",
    "\n",
    "    except ValueError:\n",
    "        print(f\"   - âŒ Validation Error: AI generated an invalid date format: '{deadline_str}'\")\n",
    "        return False # Signal failure\n",
    "    except HttpError as error:\n",
    "        print(f\"   - âŒ An API error occurred: {error}\")\n",
    "        return False # Signal failure\n",
    "\n",
    "def _mark_email_as_read(msg_id: str):\n",
    "    \"\"\"Marks an email as read with robust error handling.\"\"\"\n",
    "    try:\n",
    "        response = gmail_service.users().messages().modify(userId='me', id=msg_id, body={'removeLabelIds': ['UNREAD']}).execute()\n",
    "        if response and 'labelIds' in response and 'UNREAD' not in response['labelIds']:\n",
    "             print(f\"   - Marked email {msg_id} as read.\")\n",
    "    except HttpError as e:\n",
    "        print(f\"   - âŒ ERROR: Failed to mark email {msg_id} as read: {e}\")\n",
    "\n",
    "def fetch_unread_emails(state: AgentState) -> AgentState:\n",
    "    \"\"\"Fetches new emails and initializes the counters.\"\"\"\n",
    "    query = \"is:unread -label:Agent-Processed category:primary\"\n",
    "    results = gmail_service.users().messages().list(userId=\"me\", q=query).execute()\n",
    "    messages = results.get(\"messages\", [])\n",
    "\n",
    "    # Initialize state for the run\n",
    "    num_messages = len(messages)\n",
    "    print(f\"ðŸ“¬ Found {num_messages} new emails to process.\")\n",
    "    return {\n",
    "        \"unread_emails\": messages,\n",
    "        \"total_emails\": num_messages,\n",
    "        \"email_index\": 0 # Start counter at 0\n",
    "    }\n",
    "\n",
    "def select_next_email(state: AgentState) -> AgentState:\n",
    "    \"\"\"Selects the next email and prints a clear header.\"\"\"\n",
    "    unread_emails = state.get(\"unread_emails\", [])\n",
    "    if not unread_emails:\n",
    "        return {\"current_email_id\": None}\n",
    "\n",
    "    # Increment counter and select next email\n",
    "    email_index = state.get(\"email_index\", 0) + 1\n",
    "    total_emails = state.get(\"total_emails\", 0)\n",
    "\n",
    "    next_email = unread_emails.pop(0)\n",
    "    email_id = next_email['id']\n",
    "    details = get_email_details(email_id)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"ðŸ“§ PROCESSING EMAIL {email_index} of {total_emails}\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"   Subject: {details.get('subject')}\")\n",
    "\n",
    "    return {\n",
    "        \"unread_emails\": unread_emails,\n",
    "        \"current_email_id\": email_id,\n",
    "        \"current_email_content\": details.get(\"body\"),\n",
    "        \"current_email_sender\": details.get(\"sender\"),\n",
    "        \"current_email_subject\": details.get(\"subject\"),\n",
    "        \"current_email_message_id\": details.get(\"message_id\"),\n",
    "        \"email_index\": email_index\n",
    "    }\n",
    "\n",
    "def classify_email(state: AgentState) -> AgentState:\n",
    "    print(\"Thinking...\")\n",
    "    prompt = create_classification_prompt(state[\"current_email_content\"])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10, eos_token_id=tokenizer.eos_token_id, use_cache=False)\n",
    "\n",
    "    classification = tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True).strip().lower()\n",
    "    # Clean up messy classification output\n",
    "    if \":\" in classification:\n",
    "        classification = classification.split(\":\")[0]\n",
    "\n",
    "    print(f\"   - Classification: {classification}\")\n",
    "    return {\"classification\": classification}\n",
    "\n",
    "def extract_and_create_event(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Tool 1: Extracts deadline, validates it, creates event, and ONLY marks read on success.\n",
    "    This version includes robust error handling to prevent crashes.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ› ï¸ Using Tool: Create Calendar Event\")\n",
    "    prompt = create_extraction_prompt(state[\"current_email_content\"])\n",
    "\n",
    "    # --- 1. Generate text from the AI Model ---\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_len = inputs['input_ids'].shape[1]\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=100, eos_token_id=tokenizer.eos_token_id, use_cache=False)\n",
    "    answer = tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True).strip()\n",
    "    print(f\"   - Model extracted: {answer}\")\n",
    "\n",
    "    # --- 2. Safely process the output ---\n",
    "    success = False\n",
    "    try:\n",
    "        # ** FIX: The 'result' variable is created and used *inside* this safe block **\n",
    "        result = json.loads(answer)\n",
    "        if result and \"task\" in result and \"deadline\" in result:\n",
    "            # The helper function now tells us if it succeeded\n",
    "            success = _create_calendar_event(result[\"task\"], result[\"deadline\"])\n",
    "\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # This block runs if the AI's output is not a valid JSON, preventing the crash.\n",
    "        print(f\"   - âš ï¸ Could not parse JSON from model output.\")\n",
    "\n",
    "    # --- 3. Only mark as read if the entire process was successful ---\n",
    "    if success:\n",
    "        _mark_as_processed_and_read(state[\"current_email_id\"])\n",
    "    else:\n",
    "        print(\"   - âš ï¸ Process failed. The original email will NOT be touched.\")\n",
    "\n",
    "    return {}\n",
    "\n",
    "def draft_and_send_reply(state: AgentState) -> AgentState:\n",
    "    \"\"\"Tool 2: Drafts AND SENDS a reply for emails classified as questions.\"\"\"\n",
    "    print(\"ðŸ› ï¸ Using Tool: Draft and Send Reply\")\n",
    "\n",
    "    # 1. Create the new prompt to ask the AI for a reply\n",
    "    prompt = create_reply_prompt(state[\"current_email_content\"])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "    # 2. Generate the reply using the AI model\n",
    "    print(\"   - Thinking of a reply...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, eos_token_id=tokenizer.eos_token_id, use_cache=False)\n",
    "\n",
    "    # 3. Decode the generated text\n",
    "    reply_body = tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "    print(f\"   - Sending Generated Reply: '{reply_body}'\")\n",
    "\n",
    "    # 4. Send the dynamically generated email\n",
    "    _create_and_send_reply(\n",
    "        to=state[\"current_email_sender\"],\n",
    "        subject=state[\"current_email_subject\"],\n",
    "        in_reply_to=state[\"current_email_message_id\"],\n",
    "        reply_body=reply_body\n",
    "    )\n",
    "\n",
    "    _mark_email_as_read(state[\"current_email_id\"])\n",
    "    return {}\n",
    "\n",
    "def archive_email(state: AgentState) -> AgentState:\n",
    "    print(\" Using Tool: Archive Email\")\n",
    "    print(f\"   - Archiving email {state['current_email_id']}.\")\n",
    "    _mark_as_processed_and_read(state[\"current_email_id\"])\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1TQPfUKcdvH"
   },
   "outputs": [],
   "source": [
    "def create_reply_prompt(email_content: str) -> str:\n",
    "    \"\"\"Creates a prompt for the LLM to draft a helpful reply.\"\"\"\n",
    "    return f\"\"\"<|system|>\n",
    "You are a helpful AI assistant. Read the user's email and provide a concise and helpful response to their question.<|end|>\n",
    "<|user|>\n",
    "{email_content}<|end|>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4vVaf8WSa6L"
   },
   "outputs": [],
   "source": [
    "\n",
    "def _create_draft(to: str, subject: str, in_reply_to: str, reply_body: str):\n",
    "    \"\"\"Creates a draft reply in Gmail for human review.\"\"\"\n",
    "    print(\"   - Composing draft...\")\n",
    "    message = MIMEText(reply_body)\n",
    "    message['to'] = to\n",
    "    message['from'] = 'me'\n",
    "    message['subject'] = f\"Re: {subject}\"\n",
    "    message['In-Reply-To'] = in_reply_to\n",
    "    message['References'] = in_reply_to\n",
    "\n",
    "    encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n",
    "    body = {'message': {'raw': encoded_message}}\n",
    "\n",
    "    try:\n",
    "        draft = gmail_service.users().drafts().create(userId='me', body=body).execute()\n",
    "        print(f\"   - âœ… Success! Draft created with ID: {draft['id']}. Please review it in Gmail.\")\n",
    "    except HttpError as error:\n",
    "        print(f\"   - âŒ An API error occurred while creating the draft: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhIw21KJegJO"
   },
   "outputs": [],
   "source": [
    "def draft_reply_for_approval(state: AgentState) -> AgentState:\n",
    "    \"\"\"Tool 2: Drafts a reply for human review and approval.\"\"\"\n",
    "    print(\"ðŸ› ï¸ Using Tool: Draft Reply for Approval\")\n",
    "\n",
    "    # 1. Create the prompt to ask the AI for a reply\n",
    "    prompt = create_reply_prompt(state[\"current_email_content\"])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_len = inputs['input_ids'].shape[1]\n",
    "\n",
    "    # 2. Generate the reply using the AI model\n",
    "    print(\"   - Thinking of a reply...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, eos_token_id=tokenizer.eos_token_id, use_cache=False)\n",
    "\n",
    "    # 3. Decode the generated text\n",
    "    reply_body = tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "    print(f\"   - Generated Draft: '{reply_body}'\")\n",
    "\n",
    "    # 4. Create the draft in Gmail instead of sending\n",
    "    _create_draft(\n",
    "        to=state[\"current_email_sender\"],\n",
    "        subject=state[\"current_email_subject\"],\n",
    "        in_reply_to=state[\"current_email_message_id\"],\n",
    "        reply_body=reply_body\n",
    "    )\n",
    "    # ** FIX: Apply the 'Processed' label so we don't draft a reply again **\n",
    "    _label_as_processed(state[\"current_email_id\"])\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyWTApYY4_NL"
   },
   "source": [
    "**THE AGENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ebl9NXQ3R47U",
    "outputId": "8cbf9b69-1cb5-4724-9c3f-ff6d014988a3"
   },
   "outputs": [],
   "source": [
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"fetch_emails\", fetch_unread_emails)\n",
    "workflow.add_node(\"select_next_email\", select_next_email)\n",
    "workflow.add_node(\"classify_email\", classify_email)\n",
    "workflow.add_node(\"create_event_tool\", extract_and_create_event)\n",
    "workflow.add_node(\"draft_reply_tool\", draft_reply_for_approval)\n",
    "workflow.add_node(\"ignore_tool\", ignore_email)\n",
    "\n",
    "\n",
    "# This function is the \"brain\" of our agent, deciding what to do next.\n",
    "def router(state: AgentState):\n",
    "    print(\"ðŸ§  Agent is making a decision...\")\n",
    "    classification = state.get(\"classification\", \"\").lower()\n",
    "    if \"deadline\" in classification:\n",
    "        return \"create_event_tool\"\n",
    "    elif \"question\" in classification:\n",
    "        return \"draft_reply_tool\"\n",
    "    else:\n",
    "        return \"ignore_tool\"\n",
    "\n",
    "# 4. Define the edges and control flow\n",
    "workflow.set_entry_point(\"fetch_emails\")\n",
    "workflow.add_edge(\"fetch_emails\", \"select_next_email\")\n",
    "\n",
    "# This conditional edge determines if the loop should continue or end\n",
    "def should_continue(state: AgentState):\n",
    "    if state.get(\"current_email_id\") is None:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"classify_email\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"select_next_email\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"classify_email\": \"classify_email\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# This is the main decision-making edge, powered by our router\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_email\",\n",
    "    router,\n",
    "    {\n",
    "        \"create_event_tool\": \"create_event_tool\",\n",
    "        \"draft_reply_tool\": \"draft_reply_tool\",\n",
    "        \"ignore_tool\": \"ignore_tool\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# After a tool is used, the agent loops back to process the next email\n",
    "workflow.add_edge(\"create_event_tool\", \"select_next_email\")\n",
    "workflow.add_edge(\"draft_reply_tool\", \"select_next_email\")\n",
    "workflow.add_edge(\"ignore_tool\", \"select_next_email\")\n",
    "\n",
    "# 5. Compile and run the agent\n",
    "app = workflow.compile()\n",
    "print(\"ðŸš€ Agent compiled. Starting main loop...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(\"\\n\\n\" + \"=\"*60)\n",
    "        print(f\"ðŸš€ STARTING AGENT RUN at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        app.invoke({})\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"âœ… Run Complete. Agent is now sleeping.\")\n",
    "        print(\"=\"*60)\n",
    "        time.sleep(30)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nðŸš« Agent run manually stopped. Exiting.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ’¥ A critical error occurred: {e}\")\n",
    "        time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofcB_z9CekUR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HsHQferIuv89",
    "outputId": "aee905f1-6b78-445e-9f54-69e30c343400"
   },
   "outputs": [],
   "source": [
    "!pip install -q scikit-learn sentence-transformers\n",
    "\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def run_evaluation():\n",
    "    \"\"\"\n",
    "    Runs an advanced evaluation, calculating F1 Score and Task Success Rate.\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ Starting Advanced Evaluation...\")\n",
    "\n",
    "    # --- 1. Load Models and Test Data ---\n",
    "    print(\"   - Loading similarity model...\")\n",
    "    try:\n",
    "        similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    except Exception as e:\n",
    "        print(f\"   - âŒ Could not load similarity model. Please check your connection. Error: {e}\")\n",
    "        return\n",
    "    print(\"   - Model loaded.\")\n",
    "\n",
    "    test_file = \"test_emails.jsonl\"\n",
    "    if not os.path.exists(test_file):\n",
    "        print(f\"âŒ Evaluation file not found: {test_file}\")\n",
    "        return\n",
    "\n",
    "    test_data = []\n",
    "    with open(test_file, 'r') as f:\n",
    "        for line in f:\n",
    "            test_data.append(json.loads(line))\n",
    "\n",
    "    # --- 2. Initialize Metrics and Storage for Scores ---\n",
    "    successful_tasks = 0\n",
    "    ground_truth_classifications = []\n",
    "    predicted_classifications = []\n",
    "\n",
    "    deadline_emails_total = 0\n",
    "    deadline_extraction_success = 0\n",
    "    deadline_values_correct = 0\n",
    "\n",
    "    question_emails_total = 0\n",
    "    similarity_scores = []\n",
    "\n",
    "    # --- 3. Loop Through All Test Emails ---\n",
    "    for item in tqdm(test_data, desc=\"Evaluating Emails\"):\n",
    "        email_text = item['text']\n",
    "        ground_truth = item['ground_truth']\n",
    "        task_is_successful = False\n",
    "\n",
    "        # --- Test Classification ---\n",
    "        prompt = create_classification_prompt(email_text)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        prompt_len = inputs['input_ids'].shape[1]\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=10, eos_token_id=tokenizer.eos_token_id, use_cache=False)\n",
    "\n",
    "        prediction = tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True).strip().lower()\n",
    "\n",
    "        # Clean the prediction for accurate scoring\n",
    "        if \"deadline\" in prediction: predicted_class = \"deadline\"\n",
    "        elif \"question\" in prediction: predicted_class = \"question\"\n",
    "        else: predicted_class = \"other\"\n",
    "\n",
    "        ground_truth_classifications.append(ground_truth['classification'])\n",
    "        predicted_classifications.append(predicted_class)\n",
    "        is_classification_correct = (predicted_class == ground_truth['classification'])\n",
    "\n",
    "        # --- Test Tools and Determine End-to-End Task Success ---\n",
    "        if ground_truth['classification'] == 'deadline':\n",
    "            deadline_emails_total += 1\n",
    "            prompt = create_extraction_prompt(email_text)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            prompt_len = inputs['input_ids'].shape[1]\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=100, eos_token_id=tokenizer.eos_token_id, use_cache=False)\n",
    "            extracted_text = tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "            try:\n",
    "                extracted_json = json.loads(extracted_text)\n",
    "                if isinstance(extracted_json, dict):\n",
    "                    deadline_extraction_success += 1\n",
    "                    if (extracted_json.get('task') == ground_truth.get('task') and extracted_json.get('deadline') == ground_truth.get('deadline')):\n",
    "                        deadline_values_correct += 1\n",
    "                        if is_classification_correct:\n",
    "                            task_is_successful = True\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass\n",
    "\n",
    "        elif ground_truth['classification'] == 'question':\n",
    "            question_emails_total += 1\n",
    "            reply_prompt = create_reply_prompt(email_text)\n",
    "            inputs = tokenizer(reply_prompt, return_tensors=\"pt\").to(model.device)\n",
    "            prompt_len = inputs['input_ids'].shape[1]\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=150, eos_token_id=tokenizer.eos_token_id, use_cache=False)\n",
    "            agent_reply = tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True).strip()\n",
    "            golden_reply = ground_truth.get(\"golden_reply\")\n",
    "            if agent_reply and golden_reply:\n",
    "                embedding1 = similarity_model.encode(agent_reply, convert_to_tensor=True)\n",
    "                embedding2 = similarity_model.encode(golden_reply, convert_to_tensor=True)\n",
    "                cosine_score = util.cos_sim(embedding1, embedding2).item()\n",
    "                similarity_scores.append(cosine_score)\n",
    "                if is_classification_correct and cosine_score > 0.75: # Success threshold\n",
    "                    task_is_successful = True\n",
    "\n",
    "        elif ground_truth['classification'] == 'other':\n",
    "            if is_classification_correct:\n",
    "                task_is_successful = True\n",
    "\n",
    "        if task_is_successful:\n",
    "            successful_tasks += 1\n",
    "\n",
    "    # --- 4. Calculate and Print Final Results ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ“Š ADVANCED EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # F1 Score for Classification\n",
    "    f1 = f1_score(ground_truth_classifications, predicted_classifications, average=\"weighted\")\n",
    "    print(f\"\\nRouting Performance (Classification):\")\n",
    "    print(f\"  - F1 Score (Weighted): {f1:.2f}\")\n",
    "    print(\"     (A score combining precision and recall. 1.0 is perfect.)\")\n",
    "\n",
    "    # Task Success Rate (End-to-End)\n",
    "    task_success_rate = (successful_tasks / len(test_data)) * 100\n",
    "    print(f\"\\nAgent End-to-End Performance:\")\n",
    "    print(f\"  - Task Success Rate: {task_success_rate:.1f}% ({successful_tasks} of {len(test_data)} tasks completed perfectly)\")\n",
    "    print(\"     (Measures if the agent did the right thing from start to finish.)\")\n",
    "\n",
    "    # Semantic Similarity for Replies\n",
    "    if question_emails_total > 0:\n",
    "        avg_similarity = (sum(similarity_scores) / len(similarity_scores)) * 100 if similarity_scores else 0\n",
    "        print(f\"\\nReply Generation Quality (for 'question' emails):\")\n",
    "        print(f\"  - Average Semantic Similarity: {avg_similarity:.2f}%\")\n",
    "        print(\"     (Measures how close in meaning replies are to the ideal answers.)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# --- Run the evaluation ---\n",
    "# Ensure your model, tokenizer, and prompt functions are defined before running this.\n",
    "run_evaluation()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
